/**

\page TutorialParallelization Parallelization: running SKIRT on multiple cores and/or nodes

During this tutorial you will learn about how SKIRT is adapted for parallel computing and how to put running parallel SKIRT simulations in practice. First, an short introduction will be given into the concepts of parallel computations and parallel programming. Then, you will learn to use multithreading to simulate a panchromatic model of a spiral galaxy on your own laptop, and interpret the performance and output. Then, you will be introduced to the use of MPI and hybrid parallelization modes for SKIRT. You will also be shown how to compose a job or batch script for performing SKIRT simulations on high performance computing systems. Then, using the output of a series of simulations of the spiral galaxy model already performed on a computing cluster, you will learn how to asses the scaling behaviour of the performance and memory usage. The load balancing and its effect on the scaling will also be studied, as well as the the effect of parallelization overheads such as communication.

\section TutorialParallelizationIntroduction Introduction

\subsection TutorialParallelizationIntroductionParallelComputing Parallel computing

For the first 30 years that computers were used, programs were sequential or \em serial in nature. This means that the instructions given to the computer were executed one after the other, on a single processor. A sequence or a set of instructions is called an \em algorithm.

In the 1980’s, when multiprocessor computers began to arise, people realized the benefits of executing computer programs in parallel. Parallel computing means that multiple instructions are executed at the same time, by different processors. This is done by first breaking up the problem into smaller bits (\em tasks), which can be solved independently of each other. These tasks are then assigned to different processors, who carry out their instructions sequentially. This is illustrated in the following figure.

\image latex parallel.png "" width=5cm

It is the task of the programmer to find a way to break up the problem into pieces that can be run independently. It is generally not possible to parallelize an entire program: some parts of an algorithm are too connected to divide them into seperate tasks. These parts make up the \em serial part of the program. The other part is called the \em parallel part. Minimizing the serial part is very important too achieve significant performance benefits from the parallelization. This, however, can be a hard problem to crack. On top of that, most parallel programs require some form of information exchange between processors. It is often the case that between the execution of two pieces of work on a processor, information is needed from the other processors. Problems which require none or a very small amount of communication between processors are called \em embarrassingly \em parallel. For these problems, only little effort is needed to break up the problem in independent, parallel pieces. The opposite of an embarrassingly parallel problem is an \em inherently \em serial problem, which can not (in any reasonable way) be split in parallel pieces.

Problems can be split up in independent pieces in two different ways. These are called \em decomposition or \em partitioning methods. The first kind of decomposition is called \em domain \em decomposition or \em data parallelization. Suppose that we want to apply some set of operations on a two-dimensional matrix of data, as illustrated in the figure below.

\image latex data.png "" width=10cm

If we have multiple processors available, we can partition this two-dimensional domain so that each processor can execute the same instructions on a subset of the data. The algorithm would now require only a fraction of the time to complete. Additionally, each processor has to store less information in its memory, which could lead to larger problems being able to be solved. The principle of data parallelization with 3 parallel processors is illustrated below.

\image latex dataparallel.png "" width=10cm

An other form of decomposition is called \em functional \em decomposition or \em task \em parallelization. In some way, it can be seen as the opposite of data parallelization. Instead of executing the same operations on different parts of the data, the processors perform a different subset of the required operations. This is illustrated in the next figure. Generally, these processes use the same data, but sometimes they require different parts of the data because the instructions dictate so.

\image latex taskparallel.png "" width=10cm

With each process only executing a part of the algorithm, it is obvious that also task parallelization can lead to significant speedups. As opposed to data parallelization, pure task parallelization does not necessary lead to fewer memory consumption per processor. An advantage of task parallelization is however that it is generally easier to implement than data parallelization. It is important to understand that most parallel programs use a form of both data parallelization and task parallelization: the one does not exclude the other, and in fact both parallelization mechanisms can often be efficiently combined.

\subsection TutorialParallelizationSharedDistributed Shared and distributed memory

In a multiprocessor system, processing cores and memory modules can be arranged in different ways. A system where all cores are connected to the same memory is called a \em shared-memory system. An arrangement of cores, each with their own seperate memory, is a \em distributed-memory system.

On a shared-memory system, it is beneficial if computations that run in parallel on multiple cores can share resources. This can be achieved by \em multithreaded programming, which allows for multiple execution \em threads to read from and write to the same data.

The strength of multithreaded parallelization, however, is also its weakness: since threads share memory, \em contentions for the same resource increase rapidly in frequency as more cores (threads) are added. Exclusively multithreaded applications are therefore severely limited in terms of performance scaling.

When there is too much contention for the same resources or too much load on interconnecting memory buses, \em distributed \em memory \em parallelization provides a solution. With this model, resources are kept private to each execution unit (which is called a process in this case). The advantage of this method is that since none of the memory is shared, there is no contention, and additionally, processes can allocate all their memory locally without affecting the load on interconnecting busses.

Real-world computing systems often do not have a purely shared- or distributed memory topology, which limits the applicability of multithreading and multiprocessing and demands a more flexible parallelization strategy. To illustrate this, a diagram is shown below of a computing cluster consisting of 3 nodes connected through a network.

\image html figures/cluster.pdf
\image latex figures/cluster.pdf "" width=10cm

On the figure, look how logical units (cores) and memory units (RAM) are arranged. Is all memory equally available to all cores?
Note how this system has a distributed-memory topology on the highest level (nodes have their own memory), but each node in itself is a shared-mory system. On a system such as this, neither multithreading nor multiprocessing will be the best solution, but rather the combination of both. This is called \em hybrid \em paralleliation.

\subsection TutorialParallelizationIntroducationMCRT Parallelization in MCRT

As the applications of dust radiative transfer have been progressing towards higher-resolution output, high optical depth models and tackling complex physical phenomena such as polarization and stochastic heating of small dust grains, the performance of radiative transfer codes has become increasingly important. Despite the various acceleration mechanisms developed for MC radiative transfer, the ever-increasing demand for computing resources of radiative transfer applications require the exploitation of massively parallel systems (such as high-performance computing clusters).

MCRT codes are inherently different from grid-based solvers such as hydrodynamical codes, practically all of which are already parallelized. These codes lend themselves to massive parallelization natively since each processing core can be assigned to one particular part of the computational domain, for the entire course of the program. Communications between processing cores are generally nearest-neighbour, and global communications are almost never required. In radiative transfer, a completely different approach is needed. The MCRT algorithm works by launching numerous photon packages with a certain luminosity through the dusty system and determining random interaction positions. The properties of each individual photon package are therefore dispersed throughout the entire system, with the location of the absorption and scattering events effectively unpredictable. Equivalently, following a specific photon packages thus requires information about the dust system over the complete physical domain.

The nature of the MCRT algorithm (shooting a great amount of independent photon packages through the dust system) lends itself naturally to a task-based approach rather than a (physical) domain decomposition in the same sense as what grid-based solvers use. However, data parallelization can still be achieved by decomposing in wavelength space in some parts of the algorithm, and decomposing in physical space in other parts. The parallelization in SKIRT has been implemented according to this principle, which will be explained below.

\subsection TutorialParallelizationIntroductionSKIRT Parallelization in SKIRT

In SKIRT, we have created a novel approach to the problem of parallelization in radiative transfer codes that is efficient and flexible enough to run a wide variety of models on a wide variety of architectures. We have implemented a hybrid programming model, combining both shared-memory and distributed-memory parallelization. In this way, many of the problems that each of these methods have are alleviated, becasue the program can adapt to the specific properties of the system it is run on.

We created an algorithm that is decomposed in wavelength space in some parts, and decomposed in physical space in other parts and prove that the necessary shifting around of data is efficient.

Below is a diagram that gives an overview of the different steps that are traversed during a typical SKIRT simulation. The left column shows the main simulation phases. In green are the distinct sub-phases that are relevant for the Monte Carlo algorithm. The column of purple boxes illustrates the main data structures that are maintained in memory during a SKIRT simulation. These include the instruments, two 2D tables (dimension \f$N_{cells} \times N_{\lambda}\f$) for the absorbed luminosities in the dust system, and a 2D table (also \f$N_{cells} \times N_{\lambda}\f$) for the dust emission spectra.


\image latex skirtflow.pdf "" width=15cm


The parallelized parts of the algorithm are all the "photon shooting" and "dust emission spectra calculation" steps, so all green boxes except for those during he setup and the writing phase. The setup and writing phases form the serial part of the algorithm.

The task parallelization in SKIRT is achieved by a combination of assigning photon packages to the execution units during a photon shooting phase, and assigning dust cells to the execution units during a dust emission spectra calculation. In other words, the tasks correspond to photon packages and dust cells respectively.

Depending on whether the computing units consist only of threads or a combination of processes and threads, we make the dinstinction between two parallelization modes in SKIRT: the \em multithreading mode and the \em hybrid mode. Depending on whether the data parallelization is also enabled, the hybrid mode can be split up further into the \em hybrid \em task \em parallel mode and the \em hybrid \em task+data \em parallel mode. These 3 distinct modes are discussed below.

1. Multithreading mode
----------------------

In multithreaded or singleprocessing mode, only one process is launched (no MPI), so all work is divided amongst memory-sharing threads. Obviously, this setup requires a shared-memory system (such as a single node). Pure multithreading in SKIRT is limited in scalability up to 8-12 cores. Beyond this, speedup are marginal and whatever additional gain doesn’t justify the extra occupation of resources.

Since this is a form of shared-memory programming, no explicit communications have to be implemented between the execution units: threads can already access the same data structures, stored only in one copy in memory.

2. Hybrid task parallel mode
----------------------------

In hybrid task parallel mode, the multithreading capabilities are complemented with the distributed-memory component (MPI). This mode is useful for simulations too computationally demanding to be run on a personal computer or on a single node. A downside of this method is that the simulation memory is replicated on each process, which drives up the memory consumption on the computing nodes.

\image html figures/hybrid.pdf
\image latex figures/hybrid.pdf "" width=10cm

For the hybrid parallelization, communications have to be implemented that synchronize the data structures (absorption luminosities and dust emission spectra) across the processes. In hybrid task parallel mode, this is achieved with a collective summation; 'stacking' the 2D tables from each process and summing them element-wise.

3. Hybrid task+data parallel mode
---------------------------------

To solve issues with memory consumption for high-resolution simulations, the hybrid task+data parallelization was implemented as an extension of the 'plain' hybrid task parallelization. In this mode, the computational domain of the simulation is split up along the wavelength dimension (at some stages of the simulation) and along the physical (dust cell) domain (during other stages of the program).

In this mode, communication is also necessary between processes, in order to achieve the representation change from wavelength-based to dust cell-based data storage. This is in contrast with the collective summation required in task parallel mode. The figure below illustrates this difference. Shown here is the representation of the 2D table of absorption luminosities at a particular process. Note the representation change from wavelength-based to dust cell-based data parallelization during the simulation. This happens multiple times during a simulation.

\image html figures/absorptiontaskdata.pdf
\image latex figures/absorptiontaskdata.pdf "" width=10cm

It is seen from this figure that - except during the representation change - the memory requirement on each process is drastically reduced in hybrid task+data parallel mode, especially when many processes are used. Specifically, the memory requirement per process is reduced by a factor \f$1/N_p\f$.

\section TutorialParallelizationStart Getting ready

To complete this tutorial, you will need an internet connection, and you will of course need a working installation of the latest SKIRT version (7.4) on your laptop. If you don't have this yet, follow the instructions on http://www.skirt.ugent.be/skirt/_installation_guide.html .

Another prerequisite for this tutorial is the Python Toolkit for SKIRT, PTS. For installation instructions, follow http://www.skirt.ugent.be/pts/_installation_guide.html. Even if you already have PTS installed, it is very important for this tutorial that you update it to the absolute latest version.

An optional dependency for this tutorial is an MPI installation. If you don't have this yet and you want to see for yourself how MPI can be applied for SKIRT, you can get it from https://www.open-mpi.org/software/ompi/v2.0/ . You will have to compile the source code yourself.

\section TutorialParallelizationSki Getting the ski file

During this tutorial, we will use a panchromatic simulation of a spiral galaxy (Sersic bulge, exponential disk, ring and 2 spiral arms). The wavelength grid consists of 160 points from 0.1 micron to 1000 micron, and the number of dust cells is 256000. The simulation contains an instrument that writes out an SED and a datacube of \f$1024 \times 1024\f$ pixels.

Download the ski file for this simulation from http://www.skirt.ugent.be/skirtdays2016/downloads/ to a new empty directory (e.g. SKIRTdays_parallelization).

\section TutorialParallelizationMultithreading Using multiple cores with multithreading

Each laptop nowadays contains multiple CPU cores. SKIRT can use these cores efficiently and reduce runtime by spawning threads (multithreading). You can specifiy the number of threads for SKIRT to use with the <tt>-t</tt> option. Navigate (<tt>cd</tt>) to the directory with the ski file and execute the following commands to create a new output directory and run SKIRT:

\verbatim
mkdir out_2
skirt -t 2 spiral.ski -o out_2
\endverbatim

While the simulation is running, search for the start of the simulation setup in the terminal output. The first two lines of the setup show the initialization of the random number generation. These lines are shown in the screenshot below.

\image html screens/random2.png
\image latex screens/random2.png "" width=10cm

From these lines (which also appear in the log file), it is always possible to infer the number of threads a simulation has been run with.

Now we will start a second terminal window and start the same simulation, but now without specifying the number of threads.
Create an output directory for the simulation (mkdir out_a), and now run SKIRT as follows:

\verbatim
skirt spiral.ski
\endverbatim

What is the number of threads that is used now? Now look for the number of logical cores on your machine. On Linux, you can use the <tt>lscpu</tt> command (see CPU(s)), and on MacOS you can use <tt>sysctl -n hw.ncpu</tt>. Compare the number of threads used by SKIRT to the number of logical cores.

From both simulations, look for the total runtime (at the end of the simulation) and write it down.

To demonstrate the benefit of using multiple cores, now start the same simulation but with only 1 thread:

\verbatim
mkdir out_1
skirt -t 1 spiral.ski -o out_1
\endverbatim

Record the total runtime and compare with the runtimes recorded before.

Now launch the simulation again with the maximum number of threads, and let the output be written to a new directory (out_b).
Now compare the output of the different runs (open the spiral_earth_total.fits files). Especially compare the datacubes of the two simulations with the same number of threads (out_a and out_b).
What do you see and what did you expect?

Due to all kinds of reasons, the different execution threads will get assigned different photon packages on each individual run of SKIRT. Therefore, when using multithreading, the results of the simulation are never exactly reproducable. This would not happen if threads were assigned staticaly to photon packages (each thread also has a fixed random seed). However, because the dynamic assignment of photon packages to threads in SKIRT (threads are assigned a new task as soon as they are available), the same photon package will be executed by a different thread in most cases and thus use different random numbers during the trajectory in the dust system.

Although it almost never is, if reproducability is really necessary for a SKIRT simulation, multithreading can not be used. Luckily there is an alternative parallelization mechanism in SKIRT; multiprocessing (see next section).

Optional: also launch a simulation with twice as many threads as there are logical cores. What can you conclude from the runtime? Would it be useful to drive up the number of threads even more?

\section TutorialParallelizationMultiprocessing Using multiple cores with MPI

Multithreading is not the only way that SKIRT can delegate work to multiple cores. The performance gains you encountered on your laptop from 1 thread to 2 threads and more would not continue to last much further than 12 cores (even with an unlimited number of cores!). For better scaling of codes on systems with much more cores (supercomputers), MPI is used. MPI stands for message passing interface, it is the most widespread technique for multiprocessing. Instead of threads, the execution units are \em processes. Processes are much different than threads, the most important difference is that they do not share data. Because of this fact, they can work (almost) independently, in fact they can even be scheduled to different nodes of a computing cluster.

Most probably, you don't have the necessary libraries on your laptop to run MPI. To test this, run:

\verbatim
which mpirun
\endverbatim

This gives no output if MPI is not present on your system. If it gives you a path (e.g. /usr/local/bin/mpirun), then you have MPI installed. In this case (if MPI was already installed at the last time you compiled SKIRT), SKIRT is also compiled including the MPI libraries, which means you can test multiprocessing right away.

To launch SKIRT with 2 processes, invoke the following command:

\verbatim
mpirun -n 2 skirt -t 1 spiral.ski
\endverbatim

If a SKIRT simulation is run with multiple processes, this is also visible in the console output and log file. During the start of the simulation, just before the setup, the number of processes is always stated when MPI is used. Example:

\image html screens/mpi2.png
\image latex screens/mpi2.png "" width=15cm

Do two runs with 4 processes. Compare the results (datacubes). Is the result reproducable?

Using multiprocessing with the same number of processes leads to reproducable results in SKIRT. This is because photon packages are assigned statically to processes, in contrast to the dynamic assignment for threads. There are many technical reasons for this distinction.

\section TutorialParallelizationHybrid Hybrid parallelization schemes

A hybrid paralleliation scheme can be achieved in SKIRT simply by combining the mpirun command with the specification of the number of threads to the SKIRT executable:

\verbatim
mpirun -n 2 skirt -t 2 spiral.ski
\endverbatim

Run the above command, if you have MPI available. Check the performance. Is it comparable to an MPI run or multithreaded run with 2 processes, 3, 4, .. processes?

\section TutorialParallelizationJobs Creating and submitting SKIRT jobs

Most of the times, it works somewhat like this.

To run a job on the HPC cluster, you will need to set up a Portable Batch System (PBS) file.

\verbatim
#!/bin/sh
# Batch script for running SKIRT on a remote system\n")
#
#PBS -N name
#PBS -o output.txt
#PBS -e error.txt
#PBS -l walltime=1:30:00
#PBS -l nodes=10:ppn=16
#PBS -m bae

# Load the necessary modules
module load module_name

# Run the simulation
mpirun -n N_p skirt -t N_t spiral.ski
\endverbatim

This script is no different than other shell scripts, you can even run this same script anywhere else, but it includes some additional information about the amount of resources that you would like to have allocated for your simulation (the number of nodes and the total computation time).

In order to use the HPC compute nodes, you must first log in to one of the head nodes, hpc-login2 or hpc-login3, and submit a PBS job. The qsub command is used to submit a job to the PBS queue and to request additional resources. The qstat command is used to check on the status of a job already in the PBS queue. To simplify submitting a job, you can create a PBS script and use the qsub and qstat commands to interact with the PBS queue.

To submit your job without requesting additional resources, issue the command

\verbatim
qsub job.sh
\endverbatim

To check the job status:

\verbatim
qstat
\endverbatim

\section TutorialParallelizationGetData Getting the simulation output

We have performed a series of simulations of the spiral model on two different computing clusters. Both are part of the UGent HPC infrastructure, partner of the Flemish Supercomputer Center (VSC).

The 'Delcatty' cluster links 160 nodes, each consisting of two 8-core sockets, by a FDR InfiniBand network. Sockets are Intel E5- 2670 (2.6 GHz Sandy Bridge architecture). Each node contains 64 GB of memory.
The 'Swalot' cluster consists of 128 nodes, and a node consists of 2 10-core Intel E5-2660v3 sockets. The amount of memory per node is 128 GB.

Download the data from http://www.skirt.ugent.be/skirtdays2016/downloads/.

\section TutorialParallelizationLoadingSimulations Loading the simulations

Navigate (cd) to the downloaded directory. This directory should contain 4 subdirectories:

 - Pure scaling single-node
 - Hybrid scaling multi-node
 - Communication
 - Hybrid scaling single-node

To get an overview of the set of simulations that have been performed, enter the following command:

\verbatim
pts discover
\endverbatim

This script will look for any log file present within subdirectories of the current working directory. It groups the simulations according to the ski file that was used. You should find 35 simulation that can be identified with the spiral.ski file, which is the same model as the one you have run on your laptop. There are 8 simulations for which the corresponding ski file was not found, these were created using a slightly altered ski file. You can compare the properties of both groups of simulations such as the number of photon packages, the number of wavelengths, the number of dust cells, etc. from the terminal output. Also shown is the number of processes and threads that was used for each simulation, and whether data parallelization was enabled.

\image html screens/simulations_ski.png
\image latex screens/simulations_ski.png "" width=10cm

\image html screens/simulations_no_ski.png
\image latex screens/simulations_no_ski.png "" width=10cm

\section TutorialParallelizationSingleNode Single-node scaling

If there are any problems with the output of this command, please let someone of the SKIRT team know as it is crucial for the rest of the tutorial.

\subsection TutorialParallelizationSingleNodePure Pure multithreading and MPI parallelization

To compare the multithreading and multiprocessing, we performed a series of simulations on a single node. For an increasing number of cores (up to a full node = 16 cores), we recorded the log files of the simulation in multithreading mode, a pure MPI (multiprocessing) and a pure MPI run with data parallelization enabled.

You don't have to worry how the log files and other simulation output are organized within the various subdirectories, the PTS command we will use performs the "discover" command under the hood to detect all simulations and gather and organize all the relevant data.

To get started with this data, navigate to the "Pure scaling single-node" directory in a Terminal window. Inside this directory, create a new directory, e.g. "plot". Then execute the following command:

\verbatim
pts plot_scaling speedup,efficiency total,stellar,spectra,dust -o plot
\endverbatim

Ideal scaling happens if increasing the number of cores speeds up the program by the same factor (an effiency of 1). Comparing the scaling for the three cases, it is clear that using only multithreading is the least efficient on a full node (a speedup of 9 on 16 cores), while the MPI runs reach about the same speedup (around 11).

The speedup as a function of number of cores can be parameterized using an equation that is called \em Amdahl's law. It is given by:

\f[
S(N_c) = \frac{1}{1-p+p/N_c}
\f]

Where \f$p\f$ is the \em parallel \em fraction:

\f[
p = \frac{T_p}{T_s + T_p}
\f]

A plot of the Amdahl relation is shown in the figure below:

\image latex plots/amdahl.png "" width=10cm

As can be seen from the curve, or from the equation by letting \f$N_c \infty\f$, the speedup reaches a maximum value given by:

\f[
S_\infty = \frac{1}{1-p}
\f]

This value is the \em theoretical \em maximum \em speedup. Thus, the total runtime of the program will in the best case be equal to \f$T_s\f$, the runtime of the serial portion of the program. In this case, the parallel portion of the program is executed infinitely fast.

Look at the plots created by the plot_scaling command.

\image latex plots/speedups_total_singlenode.pdf "" width=10cm

\image latex plots/efficiencies_total_singlenode.pdf "" width=10cm

You can see that for each of the curves, corresponding to the 'pure' parallelization modes (multithreading, MPI task and MPI task+data parallelization), a curve has been fitted. These curves are described by Amdahl's law with a specific value for the parallel fraction. In the same directory, 4 data files have been created during the fitting:

 - parameters_total.dat
 - parameters_dust.dat
 - parameters_spectra.dat
 - parameters_stellar.dat

These data files contain the value of the parameter \f$p\f$ in the different cases. Compare the parallel fractions for the different phases and the parallel fraction of the total simulation. Is this what you expect?

Also inspect the plots of the scaling behaviour (speedup, efficiency) for the individual simulation phases.

\subsection TutorialParallelizationSingleNodeHybrid Hybridisation schemes

Go to the "Hybrid scaling single-node" directory and create a new "plot" directory.

\verbatim
pts plot_scaling -o plot
\endverbatim

You will see that the procedure is aborted with an error:

\image latex screens/hybridisationerror.png "" width=12cm

It tells that all simulations that were found within this directory were run with the same number of cores, so traditional scaling relations cannot be plotted as a function of number of cores. Yet, we want to asses the performance of the different hybridisation schemes. For this purpose, the 'hybridisation' mode has been created for the plotting routine. To enable it, add the '--hybridisation' flag to the plot_scaling command:

\verbatim
pts plot_scaling -o plot --hybridisation
\endverbatim

Now it works. Look at the plot of the runtimes as a function of the number of processes \f$N_p\f$. Note that the horizontal axis doesn't correspond to number of cores or any other measure of used resources. The number of cores was equal for all data points (16), but what changes is the hybridisation scheme: the combination of the number of processes and the number of threads per process.

\image latex plots/runtimes_total.pdf "" width=10cm

Also check the timeline. On the horizontal axis is the CPU time, so the total computation time taking into account all parallel cores, and on the vertical axis is the number of processes used. In an ideal scaling case, the total CPU time remains constant for an increasing number of computing resources, which leads to a linear scaling of the speedup.

\image latex plots/timeline_16_cores.pdf "" width=10cm

Evidently, you see that the total CPU time is lowest for the hybridization scheme where the total runtime is also lowest, since \f$N_c\f$ is always the same for these tests.

\subsection TutorialParallelizationSingleNodeCommunication Communication times

In the directory "communication", create a 'plot' directory and execute:

\verbatim
pts plot_scaling runtime,CPU-time communication -o plot
\endverbatim

You will get the following warning, indicating that the script could not find log files produced during a serial run (1 core). This is normally used for the normalization of the speedup and the efficiencies, but in this case it is no problem since we are only interested in the runtime and CPU time of the communication phases.

\image html screens/no_serial_timing.png
\image latex screens/no_serial_timing.png "" width=15cm

You see the following plots:

\image latex plots/runtimes_communication.pdf "" width=10cm

\image latex plots/cpu_communication.pdf "" width=10cm

You see that a curve has been plotted against the timing data points. This curve has the form

\f[
T = a + b \times N_c + c \times log(N_c)
\f]

So a function of the number of cores with a constant, a linear and a logarithmic term.
Observe the behaviour of the curves. The fitted parameters are found in the output file "parameters_timing_communication.dat":

The result is:

- MPI task:
  * a = 65.1112834857027
  * b = 0.5084684468332847
  * c = -42.96716705132733
- MPI task+data:
  * a = 7.145341928001205
  * b = 0.1981899586439446
  * c = -8.601643600429595

The transpositions from row to column representation and from column to row representation (task+data parallel mode), are significantly faster than the summations in task parallel mode. It is clear that in any case, the communication is not a bottleneck for the performance at least up to 120 cores (where the total runtime was between 11 and 13 min with 20 processes and between 2 and 5 min with 120 processes). Compare the communication times seen in the plot with the total runtimes (look in the corresponding log files).

\section TutorialParallelizationMultiNode Multi-node scaling

\subsection TutorialParallelizationMultiNodeLoadBalancing Load balancing

In the directories "s16n" (not data-parallelized) and the directory "d16n" (data-parallelized), execute the same command to plot a timeline of the individual simulations:

\verbatim
pts plot_timelines
\endverbatim

This script will create files named timeline.pdf, one in each directory. Compare both timeline plots.

\image latex plots/timeline16processes_d.pdf "" width=10cm

\image latex plots/timeline16processes.pdf "" width=10cm

Above are the timelines of the simulations on 16 nodes (corresponding to 256 cores and 32 processes) in task+data parallel mode (top) and 'plain' task parallel mode (bottom). What is seen as red and yellow bars in these graphs represents the time spent in serial parts of the simulation (setup and writing). Green, bright blue and magenta bars represent the parallel sections: emission cycles and emission spectrum calculations. It is clear from these timelines that the parallelization covers all but a very small part of the simulation runtime. Also seen (mostly in the top panel) are dark blue bars, these represent the overhead of the MPI parallelization because of load imbalance (processes that wait for other processes to complete their work). For the task parallel mode, these are insignificant, but they clearly have a negative effect on the performance in task+data parallel mode. Also note that the MPI overhead of communication (plotted in orange) is completely neglegible.

The reason for the large load imbalance is found in the fact that in task+data parallel mode, wavelengths are distributed amongst the processes. This explains why the imbalance only occurs in emission phases (green and bright blue). What is striking about these timelines is that the waiting time increases with process rank, or that the processes with the highest ranks finish their tasks (wavelength) sooner as the processes with the lowest ranks.

Think about a possible reason for this phenomenon. If you don't know the answer or want to verify your guess, ask someone from the SKIRT team.

This effect is most prominent with a large number of processes, where the number of wavelengths assigned to each process is small. This result shows that there is an upper limit on the number of processes that can be used for a particular simulation when data parallel mode is enabled. We usually use \f$N_\lambda > 10 \times N_p\f$ as a golden rule.

\subsection TutorialParallelizationMultiNodeEfficiency Efficiency scaling

Go to the "Hybrid scaling multi-node directory". Create three directories:

 - plot_efficiency
 - plot_timeline
 - plot_memory

To asses the scaling of the performance across multiple nodes, execute the following command:

\verbatim
pts plot_scaling efficiency -o plot_efficiency
\endverbatim

You will get the following figure:

\image latex plots/efficiencies_total_multinode.pdf "" width=10cm

You will see two curves, one for the efficiency of the hybrid task+data parallel mode and one for that of the hybrid task parallel mode. Based on what you have learned before, can you now explain the difference in scaling behaviour between the two modes?

\subsection TutorialParallelizationMultiNodeMemory Memory scaling

Execute the following command:

\verbatim
pts plot_scaling memory,memory-gain,total-memory total,stellar,spectra,dust,writing -o plot_memory
\endverbatim

This will plot the scaling of the \em peak memory consumption for the total simulation, the stellar emission phase, the dust emission spectra calculation, the dust emission phase and the writing phase. It will make plots of the memory consumed per process, the memory gain (the memory consumed with respect to that for a serial run with only 1 process),  and the total memory consumption (the memory summed for all processes).

You can see that for the memory per process plots, curves have been fitted of the following form:

\f[
M(N_p) = a / N_c + b + c \times N_c
\f]

This function mixes a constant term (the unparallelizable part of the data) with a linear term (memory overhead because of the MPI parallelization), and an inversily proportional term (the parallelized data). Of course the fit is only made for the data points created with data parallelized simulations. The memory consumption for the hybrid task mode is also plotted, but it is approximately constant per process (as expected) and increasing linearly with the number of processes in the plots of the total memory.

Some parameter files will be written resulting from the fitting: parameters_memory_total.dat, parameters_memory_spectra.dat, etc.
Note that the memory is well-behaved and very predictable, according to the proposed equation.

As a verification of the implementation of the data parallelization and the fit to the memory scaling, we can calculate the actual memory requirements of the parallelizable part of the data based on the simulation parameters. These parallel resources in this simulation are represented by three parallel 2D tables (two for the absorption and one for the emission spectra) and one parallel 2D datacube for the instrument.

The number of elements in each of tables amounts to \f$160 \times 256000 = 40960000\f$ or \f$40.96 \times 10^6\f$. The number of pixels in the datacube adds up to \f$1024 \times 1024 \times 160 = 167 \times 10^6\f$.

Combining these, the total number of double values necessary to represent the data is \f$287 \times 106\f$, which corresponds to \f$2.3 \times 10^9\f$ bytes. Using the fact that the number of bytes per gigabyte is \f$2^30\f$, calculate the size of the parallel data in GB. Does this correspond to what was found with the fit? (This is equal to 2.16 GB, which almost exactly corresponds to the value of 2.15 found with the fit).

The calculation we performed manually can also be done automatically with a PTS script. Navigate to the "Pure scaling single-node" directory, where the spiral.ski file resides. In the terminal window, execute:

\verbatim
pts estimate_memory spiral.ski
\endverbatim

You will see the following output:

\image latex estimate_memory.png "" width=10cm

The output will state the estimated size of the serial part of the data and the size of the parallel part of the data (in GB). How does it compare to the result of the manual calculation? The output will also show the expected memory requirement per process for different numbers of processes.

\subsection TutorialParallelizationMultiNodePlaying Playing around

Use plot_scaling to make other plots of specific things you want to visualise from the set of log files.

<b><i>Congratulations, you made it to the end of this tutorial!</i></b>

*/
