/**

\page TutorialParallelization Parallelization: running SKIRT on multiple cores and/or nodes

In this tutorial you will learn how to run SKIRT on multiple cores and/or nodes in parallel. The first section offers
an introduction to the concepts of parallel computing and \c SKIRT's parallelization capabilities. Then, you will put
things in actual practice with \c SKIRT.

The <a href="../skirt8/_concepts.html">SKIRT concepts section</a> includes a concept note that provides a more in-depth
discussion of <tt>SKIRT</tt>'s parallelization capabilities.

\section TutorialParallelIntro Introduction

\subsection TTutorialParallelIntroComputing Parallel computing

For the first 30 years that computers were used, programs were sequential or \em serial in nature. This means that the
instructions given to the computer were executed one after the other, on a single processor. A sequence or a set of
instructions is called an \em algorithm.

In the 1980â€™s, when multiprocessor computers began to arise, people realized the benefits of executing computer
programs in parallel. Parallel computing means that multiple instructions are executed at the same time, by different
processors. This is done by first breaking up the problem into smaller bits (\em tasks), which can be solved
independently of each other. These tasks are then assigned to different processors, who carry out their instructions
sequentially. This is illustrated in the following figure.

\image html parallel.png

It is the task of the programmer to find a way to break up the problem into pieces that can be run independently. It is
generally not possible to parallelize an entire program: some parts of an algorithm are too connected to divide them
into seperate tasks. These parts make up the \em serial part of the program. The other part is called the \em parallel
part. Minimizing the serial part is very important too achieve significant performance benefits from the
parallelization. This, however, can be a hard problem to crack. On top of that, most parallel programs require some
form of information exchange between processors. It is often the case that between the execution of two pieces of work
on a processor, information is needed from the other processors. Problems which require none or a very small amount of
communication between processors are called \em embarrassingly \em parallel. For these problems, only little effort is
needed to break up the problem in independent, parallel pieces. The opposite of an embarrassingly parallel problem is
an \em inherently \em serial problem, which can not (in any reasonable way) be split in parallel pieces.

Problems can be split up in independent pieces in two different ways. These are called \em decomposition or \em
partitioning methods. The first kind of decomposition is called \em domain \em decomposition or \em data
parallelization. Suppose that we want to apply some set of operations on a two-dimensional matrix of data, as
illustrated in the figure below.

\image html data.png

If we have multiple processors available, we can partition this two-dimensional domain so that each processor can
execute the same instructions on a subset of the data. The algorithm would now require only a fraction of the time to
complete. Additionally, each processor has to store less information in its memory, which could lead to larger problems
being able to be solved. The principle of data parallelization with 3 parallel processors is illustrated below.

\image html dataparallel.png

An other form of decomposition is called \em functional \em decomposition or \em task \em parallelization. In some way,
it can be seen as the opposite of data parallelization. Instead of executing the same operations on different parts of
the data, the processors perform a different subset of the required operations. This is illustrated in the next figure.
Generally, these processes use the same data, but sometimes they require different parts of the data because the
instructions dictate so.

\image html taskparallel.png

With each process only executing a part of the algorithm, it is obvious that also task parallelization can lead to
significant speedups. As opposed to data parallelization, pure task parallelization does not necessary lead to fewer
memory consumption per processor. An advantage of task parallelization is however that it is generally easier to
implement than data parallelization. It is important to understand that most parallel programs use a form of both data
parallelization and task parallelization: the one does not exclude the other, and in fact both parallelization
mechanisms can often be efficiently combined.

\subsection TutorialParallelIntroSharedDistributed Shared and distributed memory

In a multiprocessor system, processing cores and memory modules can be arranged in different ways. A system where all
cores are connected to the same memory is called a \em shared-memory system. An arrangement of cores, each with their
own seperate memory, is a \em distributed-memory system.

On a shared-memory system, it is beneficial if computations that run in parallel on multiple cores can share resources.
This can be achieved by \em multithreaded programming, which allows for multiple execution \em threads to read from and
write to the same data.

The strength of multithreaded parallelization, however, is also its weakness: since threads share memory, \em
contentions for the same resource increase rapidly in frequency as more cores (threads) are added. Exclusively
multithreaded applications are therefore severely limited in terms of performance scaling.

When there is too much contention for the same resources or too much load on interconnecting memory buses, \em
distributed \em memory \em parallelization provides a solution. With this model, resources are kept private to each
execution unit (which is called a process in this case). The advantage of this method is that since none of the memory
is shared, there is no contention, and additionally, processes can allocate all their memory locally without affecting
the load on interconnecting busses.

Real-world computing systems often do not have a purely shared- or distributed memory topology, which limits the
applicability of multithreading and multiprocessing and demands a more flexible parallelization strategy. To illustrate
this, a diagram is shown below of a computing cluster consisting of 3 nodes connected through a network.

\image html cluster.png

On the figure, look how logical units (cores) and memory units (RAM) are arranged. Is all memory equally available to
all cores? Note how this system has a distributed-memory topology on the highest level (nodes have their own memory),
but each node in itself is a shared-mory system. On a system such as this, neither multithreading nor multiprocessing
will be the best solution, but rather the combination of both. This is called \em hybrid \em parallelization.

\subsection TutorialParallelIntroMCRT Parallelization in Monte Carlo radiative transfer

As the applications of dust radiative transfer have been progressing towards higher-resolution output, high optical
depth models and tackling complex physical phenomena such as polarization and stochastic heating of small dust grains,
the performance of radiative transfer codes has become increasingly important. Despite the various acceleration
mechanisms developed for Monte Carlo radiative transfer (MCRT), the ever-increasing demand for computing resources of
radiative transfer applications require the exploitation of massively parallel systems (such as high-performance
computing clusters).

MCRT codes are inherently different from grid-based solvers such as hydrodynamical codes, practically all of which are
already parallelized. These codes lend themselves to massive parallelization natively since each processing core can be
assigned to one particular part of the computational domain, for the entire course of the program. Communications
between processing cores are generally nearest-neighbour, and global communications are almost never required. In
radiative transfer, a completely different approach is needed. The MCRT algorithm works by launching numerous photon
packages with a certain luminosity through the dusty system and determining random interaction positions. The
properties of each individual photon package are therefore dispersed throughout the entire system, with the location of
the absorption and scattering events effectively unpredictable. Equivalently, following a specific photon packages thus
requires information about the dust system over the complete physical domain.

The nature of the MCRT algorithm (shooting a great amount of independent photon packages through the dust system) lends
itself naturally to a task-based approach rather than a (physical) domain decomposition in the same sense as what
grid-based solvers use. However, data parallelization can still be achieved by decomposing in wavelength space in some
parts of the algorithm, and decomposing in physical space in other parts. The parallelization in \c SKIRT has been
implemented according to this principle, which will be explained below.

\subsection TutorialParallelIntroSKIRT Parallelization in SKIRT

\c SKIRT offers a novel approach to the problem of parallelization in radiative transfer codes that is efficient and
flexible enough to run a wide variety of models on a wide variety of architectures. We have implemented a hybrid
programming model, combining both shared-memory and distributed-memory parallelization. In this way, many of the
problems that each of these methods have are alleviated, becasue the program can adapt to the specific properties of
the system it is run on.

We created an algorithm that is decomposed in wavelength space in some parts, and decomposed in physical space in other
parts and prove that the necessary shifting around of data is efficient.

Below is a diagram that gives an overview of the different steps that are traversed during a typical \c SKIRT
simulation. The left column shows the main simulation phases. In green are the distinct sub-phases that are relevant
for the Monte Carlo algorithm. The column of purple boxes illustrates the main data structures that are maintained in
memory during a \c SKIRT simulation. These include the instruments, two 2D tables (dimension \f$N_{cells} \times
N_{\lambda}\f$) for the absorbed luminosities in the dust system, and a 2D table (also \f$N_{cells} \times
N_{\lambda}\f$) for the dust emission spectra.

\image html skirtflow.png

The parallelized parts of the algorithm are all the "photon shooting" and "dust emission spectra calculation" steps, so
all green boxes except for those during he setup and the writing phase. The setup and writing phases form the serial
part of the algorithm.

The task parallelization in \c SKIRT is achieved by a combination of assigning photon packages to the execution units
during a photon shooting phase, and assigning dust cells to the execution units during a dust emission spectra
calculation. In other words, the tasks correspond to photon packages and dust cells respectively.

Depending on whether the computing units consist only of threads or a combination of processes and threads, we make the
dinstinction between two parallelization modes in \c SKIRT: the \em multithreading mode and the \em hybrid mode.
Depending on whether the data parallelization is also enabled, the hybrid mode can be split up further into the \em
hybrid \em task \em parallel mode and the \em hybrid \em task+data \em parallel mode. These 3 distinct modes are
discussed below.

<em>Multithreading mode</em>

In multithreaded or singleprocessing mode, only one process is launched (no MPI), so all work is divided amongst
memory-sharing threads. Obviously, this setup requires a shared-memory system (such as a single node). Pure
multithreading in \c SKIRT is limited in scalability up to 8-12 cores. Beyond this, speedup are marginal and whatever
additional gain doesnâ€™t justify the extra occupation of resources.

Since this is a form of shared-memory programming, no explicit communications have to be implemented between the
execution units: threads can already access the same data structures, stored only in one copy in memory.

<em>Hybrid task parallel mode</em>

In hybrid task parallel mode, the multithreading capabilities are complemented with the distributed-memory component
(MPI). This mode is useful for simulations too computationally demanding to be run on a personal computer or on a
single node. A downside of this method is that the simulation memory is replicated on each process, which drives up the
memory consumption on the computing nodes.

\image html hybrid.png

For the hybrid parallelization, communications have to be implemented that synchronize the data structures (absorption
luminosities and dust emission spectra) across the processes. In hybrid task parallel mode, this is achieved with a
collective summation; 'stacking' the 2D tables from each process and summing them element-wise.

<em>Hybrid data parallel mode</em>

To solve issues with memory consumption for high-resolution simulations, the hybrid task+data parallelization was
implemented as an extension of the 'plain' hybrid task parallelization. In this mode, the computational domain of the
simulation is split up along the wavelength dimension (at some stages of the simulation) and along the physical (dust
cell) domain (during other stages of the program).

In this mode, communication is also necessary between processes, in order to achieve the representation change from
wavelength-based to dust cell-based data storage. This is in contrast with the collective summation required in task
parallel mode. The figure below illustrates this difference. Shown here is the representation of the 2D table of
absorption luminosities at a particular process. Note the representation change from wavelength-based to dust
cell-based data parallelization during the simulation. This happens multiple times during a simulation.

\image html absorptiontaskdata.png

It is seen from this figure that - except during the representation change - the memory requirement on each process is
drastically reduced in hybrid task+data parallel mode, especially when many processes are used.


\section TutorialParallelPractice Putting things in practice

\subsection TutorialParallelPracticeStart Getting ready

This tutorial assumes that you have completed some of the introductory <tt>SKIRT</tt> tutorials, or that you have
otherwise acquired the working knowlegde introduced there. At the very least, before starting this tutorial, you should
have installed the <tt>SKIRT</tt> project. <tt>SKIRT</tt>'s multi-threading capabilities are enabled by default. To
allow multiprocessing, however, you need to install an MPI implementation on your computer and rebuild \c SKIRT with
the MPI build option enabled. Refer to the Installation Guide for more information.

\subsection TutorialParallelPracticeSki Creating a ski file

You will need a \c SKIRT parameter file to run the examples in this tutorial. The exact model being represented does
not matter, however it is best to use a panchromatic simulation with a fairly large number of wavelengths (100 or
more), and a number of photon packages fine-tuned so that the total run-time on your computer is on the order of one
minute or so. Make sure to include a dust system and at least one instrument that outputs data cubes. Refer to the
introductory tutorials for examples of \c SKIRT models.

\subsection TutorialParallelPracticeThreads Multiple threads

Most current desktop and laptop computers have multiple processing cores. \c SKIRT can use these cores efficiently and
reduce runtime by spawning multiple parallel threads (multithreading). You can specifiy the number of threads for \c
SKIRT to use with the <tt>-t</tt> option. Navigate to the directory containing your ski file and execute the following
commands to create a new output directory and run \c SKIRT (assuming that your ski file is called <tt>spiral.ski</tt>):

\verbatim
$ mkdir out_2
$ skirt -t 2 spiral -o out_2
\endverbatim

While the simulation is running, look for the start of the simulation setup in the terminal output. The initial lines
of the setup indicate the initialization of the random number generation. The number of these initialization lines
(which also appear in the log file on disk) always matches the number of threads a \c SKIRT simulation has been run
with. For example:

\verbatim
   Constructing a simulation from ski file 'spiral.ski'...
   Starting simulation spiral...
   Starting setup...
   Initializing random number generator for thread number 0 with seed 4357...
   Initializing random number generator for thread number 1 with seed 4358...
   ...
\endverbatim

Now create a new output directory (to avoid overwriting the previous results) and perform the same simulation without
specifying the number of threads:

\verbatim
$ mkdir out_n
$ skirt spiral -o out_n
\endverbatim

What is the number of threads used now? Look up the number of logical cores on your computer. On Linux, you can use the
<tt>lscpu</tt> command, and on Mac OS X you can use <tt>sysctl -n hw.ncpu</tt>. Compare the number of threads used by
SKIRT to the number of logical cores.

To demonstrate the benefit of using multiple cores, perform the same simulation but with a single thread:

\verbatim
mkdir out_1
skirt -t 1 spiral.ski -o out_1
\endverbatim

Find the total runtimes for each of these three simulations (listed at the end of the simulation logs) and record them
in a table. If you like, you can perform simulations with a different number of threads and add the corresponding
runtimes to the table. What is the speedup in function of the number of threads? What happens when you specify more
threads than there are cores on your computer?

Now perform the simulation once more with the default number of threads, writing the output to a new directory:

\verbatim
$ mkdir out_m
$ skirt spiral -o out_m
\endverbatim

Compare the image frames generated by the different simulation runs. Pay special attention to the results of the two
simulations with the same number of threads (\c out_n and \c out_m). What do you see and what did you expect?

Because of the dynamic assignment of photon packages to threads in \c SKIRT, the intrinsic randomness of parallel
execution causes a different and unpredictable sequence of pseudo-random numbers to be used by each photon package,
which means photon packages are sent along different random paths through the dust each time the simulation is being
performed with multiple threads.

\subsection TutorialParallelPracticeProcesses Multiple processes

The performance gains obtained by multithreading do not persist beyond 12-16 parallel threads, even on computers with
many more cores. To offer better scaling on large computer systems, \c SKIRT uses MPI, which stands for Message Passing
Interface. MPI is widely available for various operating systems and is installed on all large (multi-node) computing
systems. When using MPI, the basic execution units are \em processes rather than threads. Processes run independently
of each other, and can in fact be scheduled on different nodes of a computing cluster. On the other hand, because
processes do not automatically share data, any data exchange must be explicitly handled by the programmer.

If you're using a destop or laptop computer, you probably don't have the MPI libraries installed. To test this, try:

\verbatim
$ which mpirun
\endverbatim

If this command reports a path (e.g. /usr/local/bin/mpirun), then you have MPI installed. In this case, you can build
the \c SKIRT project with the MPI build option turned on. If the command produces no output, MPI is not present on your
system an you will need to install it, or move to a different computing system. See the Installation Guide for more
information.

Assuming \c SKIRT was properly built with MPI enabled, you can launch \c SKIRT with 2 processes as follows:

\verbatim
mpirun -n 2 skirt -t 1 spiral.ski -o mpi_2a
\endverbatim

If this works properly, the \c SKIRT log (console output and log file) lists the number of processes in use at the
start of a simulation, just before setup:

\verbatim
   Constructing a simulation from ski file 'spiral.ski'...
   Starting simulation spiral with 2 processes in task parallelization mode...
   Starting setup...
   Initializing random number generator for thread number 0 with seed 4357...
\endverbatim

Perform a second simulation  with 2 processes, storing the results in a different output directory (e.g. \c mpi_2b).
Compare the image frames generated by both simulation runs. Are the results identical?

Using multiprocessing with the same number of processes leads to identical results in \c SKIRT. This is because photon
packages are assigned statically to processes, in contrast to the dynamic assignment for threads.

\subsection TutorialParallelPracticeHybrid Hybrid parallelization schemes

A hybrid parallelization scheme can be achieved in SKIRT simply by combining the mpirun command with the specification
of the number of threads to the SKIRT executable:

\verbatim
mpirun -n 2 skirt -t 2 spiral.ski
\endverbatim

Run the above command, if you have MPI available. Check the performance. Is it comparable to a multithreaded run with 4
threads?


<b><i>Congratulations, you made it to the end of this tutorial!</i></b>

*/
