/**

\page ParallelizationMPI_skirt Multi-processing in SKIRT

\section Parallelizationmodes MPI Parallelization modes

Instead of a \em master-slave concept, where one process hands out pieces of work to the other processes as they become
available, we chose the \em peer-to-peer model for the MPI parallelization of <tt>SKIRT</tt>. This means that we let
the processes work independently, without getting instructions from other processes. Unlike the way the multithreading
is implemented, the work is not divided between the processes in a dynamic way: each process needs to know what parts
of the work it needs to de beforehand.

(As explained in \ref ParallelizationThreads_skirt, multithreading in <tt>SKIRT</tt> is designed according to the
master-slave model.)

The MPI parallelization of SKIRT can be used in two modes, both having their own strongpoints and work distribution
schemes. The first mode is called the 'task-based' mode and parallelizes only the execution of the work, while making
the load balancing as good as possible. The 'data parallelization' mode on the other hand, also parallelizes the
storage of certain large sets of data, to greatly reduce the memory usage per process. To make the parallel storage of
data possible, this mode distributes the work in a different way, making sure that each process only needs the data
that is stored in its own memory pool.

Before we dive into the implementation, we will first explain the main differences between these modes. These can be
understood best by looking at the way they parallelize the photon shooting algorithm.

\subsection Parallelizationmodes_taskbased Task-based parallelization

The lifetime of a photon package, and hence the computational load, can greatly vary with its wavelength. Therefore,
the main idea behind the purely task-based parallelization, is that all of the processes will shoot some photons for
all of the wavelengths. Hence, for shooting photon packages, the implementation of this parallellization mode is
straightforward: we simply divide the total number of photon packages \f$N\f$ per wavelength by the number of parallel
processes. The resulting number of photon packages \f$N^{\prime}=N/P\f$ is used on each process, as if they were each
performing a separate simulation with a smaller amount of photons. In turn, the processes can then distribute the
photon packages amongst their parallel threads.

Because each process basically performs an independent <tt>SKIRT</tt> simulation, the same rules must apply for the
number of chunks \em within each process to obtain a good load balancing between its threads. Note that the work is
already well balanced between the processes, since they execute the same algorithm, for the same wavelengths, and with
the same number of photon packages. The amount of chunks and their size are determined in the same way as in the
non-MPI case, except for the fact that each process determines these quantities based on only a fraction of the total
number of photon packages. To recapitulate from \ref ParallelizationThreads_skirt, the equations that determine the
number and size of the chunks are given by:

\f[N_C > \frac{N^{\prime}}{10^7} \f]
\f[N_C > 10 \times \frac{T}{N_{\lambda}} \f]

\note It is important to see the difference between the total number of photon packages \f$N\f$ per wavelength and
\f$N^{\prime}\f$, used in the equations above. While the former represents the number of photon packages for the
simulation, desired by the user, the latter represents the number of photon packages simulated by each process:
\f$N^{\prime}=N/P\f$ (where \f$P\f$ is the number of processes).

From the moment the number of chunks and chunksize have been calculated, each process can carry out an independent
radiative transfer simulation of stellar photon packages. For an oligochromatic simulation, this must be followed by a
single communication phase where the fluxes received in the instruments are collected by a single process, after which
this process writes the results to file. A panchromatic simulation involves some more communication, due to the fact
that thermal emission spectra have to be calculated based on the absorption of the stellar photons. Since these photons
are simulated by different processes, their absorbed luminosities have to be sent from their corresponding process to
all other processes. Subsequently, the thermal emission calculation can be initiated on each process. This procedure of
launching photons, recording their absorption and communicating between processes is iterated for a couple of times
during the panchromatic simulation. Eventually, the same communication is needed for the instruments as with the
oligochromatic simulations.

There is however a drawback to this purely task-based approach. The instruments and the tables storing absorption data
can become quite large; in fact they often form the main contribution to the memory usage of SKIRT. When \f$P\f$ MPI
processes are used, the total memory usage will be multiplied by \f$P\f$, as all these processes need to store data for
all the wavelengths in their instruments and absorption tables. When running a simulation with a very large number of
wavelengths, this can impose a hard limit on \f$P\f$. A solution for this problem is provided by the data parallel
mode.

\note The task-based mode is activated by default when more than one process is used. The data parallelization can be
activated by providing the \c -d option on the command line.

\subsection Parallelizationmodes_datapar Data parallelization

The main purpose of the data parallel mode, is to reduce the memory usage per process by parallelizing the storage of
the instruments and absorption data. More specifically, we want each process to store this data for only a fraction of
the wavelengths. As a consequence, we can no longer use the work division described in the previous section. Instead of
shooting a fraction of the photon packages for all (\f$N_\lambda\f$) wavelengths, the processes will now shoot all of
the photon packages, but only for their own distinct subset wavelengths, about \f$N_\lambda/P\f$ per process. For the
photon shooting, it is as if there are \f$P\f$ independent simulations running, which are all using different
wavelengths. When multithreading is used in combination with multiprocessing, each process can distribute the photon
packages for these wavelengths amongst its threads.

An important aspect to dividing the work this way, is choosing the wavelengths that each process will handle. SKIRT has
a dedicated framework built in to handle the the distribution of a number of work (or data) units, called \c
ProcessAssigner. The standard way to distribute the wavelengths is by handing then out one by one, while cycling
through the processes, as dictated by the subclass called \c StaggeredAssigner. Neighbouring wavelengths will often
provide similar computational loads, resulting in 'slow' and 'fast' wavelength ranges, and a process working only on
'slow' wavelengths would always lag behind. By using the division scheme described in \c StaggeredAssigner, it is made
sure that every range of wavelengths is split up as much as possible. Note that the load will be balanced better when
there are more wavelengths to be distributed. As a rule of thumb, no severe load balancing problems should occur when

\f[N_\lambda > 10P\f]

Similarly to the task-based mode, we can use a certain number of chunks within each process to optimize the load
balancing between its threads. Because the processes can be regarded as independent simulations with roughly
\f$N^\prime_\lambda = N_\lambda/P\f$ wavelengths, the rules for the number of chunks become

\f[N_C > \frac{N}{10^7} \f]
\f[N_C > 10 \times \frac{T}{N^\prime_{\lambda}} \f]

Once the number of chunks and the chunksize have been set, each process will simulate the stellar emission for its own
wavelengths, storing the instruments and absorbed photons in much smaller data structures. For an oligochromatic
simulation, no real difficulties appear with this approach. At the end, the data contained in all the instruments is
collected by a single process, and assembled in to one large datacube (implemented by \c ParallelDataCube).

In panchromatic simulations there is another important aspect, which concerns the calculation of the thermal emission
spectra. During this phase, the dust emission is calculated starting from the absorbed stellar luminosity in each dust
cell. The problem that arises here, is that this absorption data is needed for all wavelengths, while each of the
processes only handle a subset of the wavelenghs. Simply gathering all this data would render the memory advantage
gained during the photon shooting phase useless. The solution is to change the parallelization scheme before entering
the thermal emission calculation phase. Instead of storing the absorption for certain wavelengths and for all
dustcells, we make the transition to a scheme where each process has access to the absorption for all wavelengths, but
for only a subset of the dustcells. In practice, this comes down to a complex MPI communication, which changes the
storage format of a table from 'parallelized by columns' to 'parallelized by rows'.

After this transposition, each process will be able to calculate a thermal emission spectrum for each of its dust
cells. When the thermal emission has been calculated, every process will thus contain emission spectra for a subset of
the dust cells. Before these spectra can be used for the next photon shooting phase, another transposition needs to
happen. A process will no longer need the spectra at all wavelengths, as it will only shoot photons for its own subset
of wavelengths. We do however need the data over all dust cells, to calculate the spatial distribution of the emission.
Thats why, before shooting the photons, the parallelization scheme is switched back from a dust cell format to a
wavelength format.

To summarize, two transposition operations happen per iteration of the thermal emission loop: one after the photon
shooting, and one after the thermal emission calculation. Although the code to perform these communications is quite
verbose, the actual communication times are of the same order as those used by the task-based parallelization. Most of
the complexities concerning the parallel storage of data and the transposition of the storage scheme, are encapsulated
in the \c ParallelTable class.

Aside from possible load balancing issues, there is one other drawback to using data parallelization. Normally, the
calculation of the thermal emission can be simplified and sped up by using a dust library (see \ref
emissioncalculation_library). With the current implementation, the aborption data in all dust cells is needed to
properly construct a library, and therefore \c Dim1DustLib and \c Dim2DustLib can not be used.

\note When using data parallelization, the absorption data is only available for a subset of the dust cells, and
therefore the calculation has to happen per cell individually. In practice, this means that only the \c AllCellsDustLib
is allowed.

\section MPIsupport MPI support classes

\subsection MPIsupport_processmanager The ProcessManager class

To provide a convenient interface to the MPI library, a class called ProcessManager is added to the <tt>SKIRT</tt>
code. Its source and header files are placed under a directory called MPIsupport. The ProcessManager class is the only
place in the <tt>SKIRT</tt> code where explicit calls to the MPI library are allowed. The ProcessManager source file
can be compiled with or without MPI, depending on whether the MPI compiler can be detected on the system. If it is
compiled with MPI, the MPI header is included and the appropriate calls to the MPI library are inserted. Otherwise,
these calls are ignored and the ProcessManager class always returns default values to the rest of the code (number of
processes = 1, rank of the process = 0). <tt>SKIRT</tt> can then be run as usual, without the MPI functionality.

No instances of the ProcessManager can be created. Its members include only static functions and variables. The
constructor is deleted so that the compiler ensures that it can never be called. The reason is that the ProcessManager
reflects a \em resource, being the MPI environment. Within the runtime of a program, this resource is available
<em>once and only once</em> (this is defined in the MPI standard). As soon as the MPI environment is initiated, the
rest of the code can call certain operations that perform communications between the \f$n\f$ processes. The number of
processes \f$n\f$ is \em always specified at the very moment the program launches, and can \em never be changed during
runtime. Hence, it is meaningless to create multiple objects during the course of the program, all with the purpose of
representing the same collection of processes. The ability to communicate between the processes and to inquire the n
and the process’ rank can thus indeed be seen as a solitary resource.

A solitary resource can be handed out once and further requests for it must be answered with a negative response. Only
if the resource is released again, a next request results in a positive reply. Let’s translate that in case the
resource is MPI and the way to obtain it is through the ProcessManager class. The first time ProcessManager is asked
whether an MPI environment is present, it can happily reply by stating the number of processes \f$n\f$ and the rank of
the process that requested the information. If the number \f$n\f$ is greater than one, that process knows that it is
part of a parallel program and executes the code accordingly. If \f$n=1\f$, MPI is either absent or the MPI environment
trivially consists of one process. Consequently, the entire simulation is run by this process. When a first request has
been performed, a next request to the ProcessManager will always result in a ‘non-MPI’ answer: \f$n=1\f$ and a rank of
zero. Any request thereafter results in the very same answer. The number of performed request can be conveniently
stored in a static variable of the ProcessManager class. This variable is increased with one after every request, and
decreased with one whenever such a request is released again. With every request, the value of the counter is checked.
Only if it is zero, the correct number of n processes is returned to the caller.

As we already mentioned, the MPI environment must always be initialized (once) and subsequently finalized (once) again.
Both operations can be performed by a call to resp. the <tt>initialize</tt> and <tt>finalize</tt> functions of the
ProcessManager class. The request to acquire MPI is managed by a function called <tt>acquireMPI</tt> and releasing the
MPI resource is done by the <tt>releaseMPI</tt> function. The private variable that stores the number of requests is
called requests, and is of type std::atomic<int>. Objects of the atomic type are free from data races, meaning that if
one thread writes to an atomic variable while another thread reads from it, the behaviour is well defined. In other
words, making requests an atomic int instead of a regular int avoids situations where two different threads try to
increase (calling acquireMPI) or decrease (calling releaseMPI) its value at the same time, leaving the requests counter
in an incorrect state.

\subsection MPIsupport_processcommunicators The Process Communicators

When either a Simulation or FitScheme object calls <tt>acquireMPI</tt>, it obtains a number of processes and a rank,
which it is supposed to remember for the rest of the object’s lifetime. Instead of creating new data members in the
Simulation and FitScheme class, this information is stored in an object we call a \em communicator. We added a new
abstract class, ProcessCommunicator, which acts as a base class for the classes PeerToPeerCommunicator and
MasterSlaveCommunicator. The base class provides two data members: \c _rank and \c _Nprocs. Their values are defined
during the setup (ProcessCommunicator::setupSelfBefore) of the ProcessCommunicator class, when the <tt>acquireMPI</tt>
function of ProcessManager is called. ProcessManager also defines a (virtual) destructor (calling <tt>releaseMPI</tt>)
and a few functions such as <tt>getRank</tt>, <tt>getSize</tt> and <tt>isMultiProc</tt>. The implementation of the
constructor is trivial, which involves setting the \c _rank and \c _Nprocs members to a value of -1 (specifying the
ProcessCommunicator object has not been set up).

<tt>SKIRT</tt> and <tt>FitSKIRT</tt> use a different MPI parallelization strategy. Whereas in <tt>SKIRT</tt> we try to
obtain as much independence as possible between processes (except for the occasional gathering of data),
<tt>FitSKIRT</tt> lets one process give directions to what other processes should work on. That is why we have two
separate classes, PeerToPeerCommunicator and MasterSlaveCommunicator, for <tt>SKIRT</tt> and <tt>FitSKIRT</tt>
respectively. <em>Their purpose is to implement communication functions</em>, specific for the needs of the
<tt>SKIRT</tt> and <tt>FitSKIRT</tt> design. These functions, in turn, can use a shared set of lower-level
communication functions in the ProcessManager class. The latter are supposed to call the MPI library directly and deal
with data buffers of fundamental types only.

Both the Simulation and the FitScheme class create their communicator, denoted by the member variable \c _comm, in
their constructor. They do this by calling \c new PeerToPeerCommunicator() and \c new MasterSlaveCommunicator()
respectively. The base class ProcessCommunicator inherits from SimulationItem so that a pointer to the
PeerToPeerCommunicator or MasterSlaveCommunicator can be obtained at runtime with the \c find command from other
classes in the <tt>SKIRT/FitSKIRT</tt> hierarchy.

\subsection MPIsupport_ProcessAssigner The ProcessAssigner class

The ProcessAssigner class represents objects that are used by some part of the <tt>SKIRT</tt> code to decide which
parts of the work should be assigned to which process. We have created 3 different subclasses of ProcessAssigner:
SequentialAssigner, StaggeredAssigner and RandomAssigner. Each of these subclasses implement a different mechanism for
distributing the work amongst the different processes. The constructor of a ProcessAssigner takes an integer number as
an argument. This number represents the amount of parts of work that need to be performed. Each ProcessAssigner
subclass has a certain assignment implementation in the body of its constructor, which typically obtains a pointer to
the PeerToPeerCommunicator in the simulation through the <tt>find</tt> procedure, and subsequently uses the process
rank and the size of the communicator to determine which and how many parts are assigned to which process. The
SequentialAssigner does this by splitting the range of values (from zero to the number of tasks) into contiguous pieces
of more or less the same size, and assigning each process to a distinct piece. An object of the StaggeredAssigner, on
the other hand, assigns the first tasks to the first process, the second to the second process, etc. and keeps handing
out tasks to each of the processes (in the same order) until they all have been assigned. The difference between these
two assignment schemes is illustrated by the figures below.

\image html sequentialassigner.png "Illustration of how the SequentialAssigner class distributes work amongst the different processes."
\image html staggeredassigner.png "Illustration of how the StaggeredAssigner class distributes work amongst the different processes."

A third assignment scheme, implemented in the RandomAssigner, assigns each part of work to a randomly chosen process.
While this type of assigner is currently not used in the code, it provides a good test for the robustness and
generality of the parallelization.

Once constructed, a ProcessAssigner object can work together with two other classes called \c Parallel and
ParallelTable, to parallelize the execution of work and the storage of data over the different MPI processes,
respectively.

\subsection MPIsupport_parallel The Parallel class

As explained in \ref ParallelizationThreads_classes and in its documentation, the Parallel class was originally
designed as a relatively safe way of using multithreading. A ParallelTarget could be given to the \c call() function,
and the latter would execute the \c body() function of this target in a multithreaded way. To combine MPI
multiprocessing with multithreading, this class has been extended with a version of its \c call() function that takes a
pointer to a \c ProcessAssigner as an argument. Using the given ProcessAssigner, the number of work units to be
executed by the calling process can be determined through its \c nvalues() function. The \c call() function of Parallel
will then perform a multithreaded for loop with an index \c i that runs from 0 to \c nvalues(), and call the body
function on the indices that have been assigned to the calling process by the \c ProcessAssigner. These indices are
retrieved using \c ProcessAssigner::absoluteIndex(i). To parallelize 1000 executions of a \c body function over both
processes and threads, the code will look like the example below.

\code
void Test::body(size_t index) { ... }
...
SequentialAssigner assigner(1000, this);
ParallelFactory factory;
...
factory.parallel()->call(this, &Test::body, &assigner);
\endcode

The \c this argument in the contructor of the SequentialAssigner is added because a ProcessAssigner needs to link
itself into the simulation hierarchy. It will use \c this as a parent, and once it is linked in it will be able to find
the \c PeerToPeerCommunicator of the simulation and use it to perform its assignment operation. In the \c call()
function, \c this is used as the ParallelTarget, resulting in a parallel execution of \c this->body().

\subsection MPIsupport_paralleltable The ParallelTable class

To encapsulate several aspects of the data parallelization, the ParallelTable class has been created. It most important
features are:

-# The contained 2D data can be distributed over the processes, either by letting each process store a subset of the
columns, or by letting each process store a subset of the rows. It is possible to switch between these two distributed
storage formats by calling the \c switchScheme function.

-# It can be used both in a non-parallelized and parallelized way. This severely reduces the number of times we
explicitly have to check whether the data parallelization option is turned on, by unifying both ways of storing data
into one class. The way the ParallelTable is used depends on which overload of its \c initialize function is used. The
non-parallel version takes two integers among its arguments, which simply specify the required dimensions. The version
that activates the parallel storage of the data, takes two \c ProcessAssigners as arguments: one for the columns, and
one for the rows. Just like they determine the way work is distributed when given to Parallel::call(), the same
framework can be used to indicate which subset of the columns or rows should be stored. The dimensions of the table
will be equal to the integers given to the assigner objects at construction.

-# When getting an element, no explicit index conversions have to be done by the client, as the ParallelTable will
perform these internally. The client can treat the paralleltable as a regular 2D Table, using the \c ()-operator. When
one tries to retrieve an element that is not stored on a certain process, an error will be thrown.

Shown below is a piece of example code, showing the general flow of operations for a ParallelTable and the surrounding
framework, when coupled with operations executed via \c Parallel::call(). By passing the correct assigners to both
Parallel::call() and ParallelTable::initialize(), it can be ensured that only the available elements are accessed.

\code
// A 2D container that will be worked with in parallel
ParallelTable _pTable;

// Functions that use _pTable
void Test::writeColumns(size_t c) { /* body that writes data on column 'c' of height 1000 */ }
void Test::readRows(size_t r) { /* body that reads data from row 'r' of length 200 */ }

// Parallellize the writing of 200 columns
ProcessAssigner* columnAssigner = new SequentialAssigner(200, this);

// Parallelize the reading of 1000 rows
ProcessAssigner* rowAssigner = new StaggeredAssigner(1000, this);

// Parallelize the storage in exactly the same way
_pTable.initialize("Name", ParallelTable::WriteState::COLUMN, columnAssigner, rowAssigner,
                                        find<PeerToPeerCommunicator>());

// Do the writing in parallel
ParallelFactory factory;
Parallel* parallel = factory.parallel()
parallel->call(this, &Test::writeColumns, columnAssigner);

// Transpose the parallelization scheme
_pTable.switchScheme()

// Do the reading in parallel
parallel->call(this, &Test::readRows, rowAssigner);
\endcode

Now that we know these basic concepts about the ParallelTable, we can move on to some implementation details concerning
the MPI parallelization of different phases of the simulation.

\note This explanation of ParallelTable only mentions the basics. There are a lot more functions that are not mentioned
in this article, with certain subtleties caused by the fact that only a subset of the stored data is actually
present at a process. Functions often behave differently depending on the state of the ParallelTable object. For
detailed information about how, when and in what order the functions of this class can be used, please refer to the
ParallelTable documentation.

\section MPIimplementation Implementation of MPI parallelization

\subsection MPIimplementation_skirtsimulation A SKIRT simulation

A <tt>SKIRT</tt> simulation is represented by an object of either the class OligoMonteCarloSimulation or the class
PanMonteCarloSimulation, depending on the type of simulation. Both classes inherit from the MonteCarloSimulation class,
which in turn inherits from the Simulation class. The simulation is started by invoking the \c setupAndRun function,
implemented in Simulation. This function calls two other functions of Simulation, \c setup and \c run. The \c setup
function iterates over all objects in the simulation hierarchy, creating and initializing their data structures. It is
in this phase that for example the dust system (class DustSystem) calculates the volumes and densities of the dust
cells, the random generator (class Random) generates the random sequences for the different threads, the wavelength
grid (class OligoWavelengthGrid and PanWavelengthGrid) calculates the wavelength bin widths, etc.

After the setup, the \c run function of Simulation is invoked. This function executes the \c runSelf function of either
OligoMonteCarloSimulation or PanMonteCarloSimulation. In the former, this function invokes the <em>stellar emission
phase</em> and the <em>writing phase</em>. In the latter, this function invokes the <em>stellar emission</em>, <em>dust
selfabsorption</em>, <em>dust emission</em> and <em>writing</em> phases. The \c runstellaremission function,
implemented by MonteCarloSimulation, calculates the number and the size of the chunks and then calls the \c
dostellaremissionchunk function for each chunk (with or without parallel threads). The chunk number and size
calculation is done by a separate function, \c setChunkParams. As explained in \ref Parallelizationmodes, the
implementation is slightly different depending on the mode used. Details on the actual implementation can be found in
the documentation of \c MonteCarloSimulation::setChunkParams().

For the oligochromatic simulations, no other noteworthy changes were made to the <tt>SKIRT</tt> algorithm, except for
the communication in the writing phase (see \ref MPIimplementation_instruments) and a minor change in the Random class
explained in the next subsection. For the panchromatic simulations, more changes were necessary, related to the
calculation of the dust emission spectra. This is explained in \ref MPIimplementation_emissioncalculation.

\subsection MPIimplementation_randomness Randomness

The construction of some types of dust grids (e.g. an octree grid), during the setup of the simulation, depends on
random numbers. Because of the dynamical way in which threads assign themselves to available work (see \ref
ParallelizationThreads_skirt), this leads to the fact that two consecutive runs of a <tt>SKIRT</tt> simulation with the
same number of threads will almost never have an identical dust grid (for those types). Since the processes in a MPI
parallelized <tt>SKIRT</tt> simulation are essentially independent <tt>SKIRT</tt> simulations, this would result in
inconsistent grids across the processes and the program would most likely crash during some communication operation.
Remember, in panchromatic simulations, we require the processes to send the absorbed luminosities in each dust cell to
the other processes. If the dust grids are organized in a different way across processes, with differing number of
cells, this communication is doomed to fail. Even in oligochromatic simulations, we want the different processes to
simulate \em exactly the same system. Therefore, we need two conditions for the MPI parallelization regarding the setup
of the <tt>SKIRT</tt> simulations. Firstly, the parts of the setup that use random numbers can only be singlethreaded.
Secondly, these single threads must have the same random sequence on all processes. Initially, the second condition is
always fulfilled since the random seed is read in from the \em ski file or otherwise given a default, hardcoded value,
and is hence the same on all processes.

For the actual simulation of photon packages, however, we need to give each thread, <em>across all processes, a
different</em> random seed. This is to prevent of course that the thread with the same index (and seed) is assigned to
the first wavelength on two different processes, after which the two threads will essentially simulate identical
photons. This procedure is implemented by the <tt>randomize</tt> function in the Random class. This function is called
from Simulation, between the execution of setup and run. The name is obvious because without this function, the threads
with the same index would have exactly the same sequence on all processes.

Schematically, the algorithm of the randomize function goes as follows:

-# Find the PeerToPeerCommunicator within the simulation hierarchy.

-# Obtain the number of parallel threads in each process from the ParallelFactory object. A pointer to the parallel
factory is stored in a member called <tt>_parfac</tt>.

-# Give the first thread of each process a different seed, shift these seeds by exactly the number of threads. Store
the seed in the member <tt>_seed</tt> at each process.

-# Execute the algorithm that is also used in the setup of the Random class, to create a random sequence for all the
threads. The random sequence of thread zero is created from the _seed variable, the random sequence of thread 1 is
created from incrementing <tt>_seed</tt> by 1, and so on. Thus for \f$T\f$ threads, a set of \f$T\f$ consecutive seeds
are used. Since the value of <tt>_seed</tt> is shifted by \f$T\f$ on each subsequent process, the \f$P \times T\f$
different threads of the program have \f$P \times T\f$ different seeds.

This procedure is also illustrated in the figure below.

\image html randomize.png "The randomize function makes sure that each process ‘reserves’ a unique set of random seeds for its own threads."

The implementation of the Random::randomize function is listed below.

\code
void Random::randomize()
{
    PeerToPeerCommunicator* comm = find<PeerToPeerCommunicator>();

    int Nthreads = _parfac->maxThreadCount();
    _mtv.resize(Nthreads);      // Because the number of threads can be different during
    _mtiv.resize(Nthreads);     // and after the setup of the simulation.

    _seed = _seed + Nthreads * comm->rank();

    initialize(Nthreads);
}
\endcode

Because in the last procedure, we need to reuse code of the <tt>setupSelfBefore</tt> function of the Random class, we
have moved this piece of code to a separate (<em>protected</em>) function, which is called <tt>initialize</tt>. As an
argument, this function needs the number of threads, obtained from the parallel factory. Internally, it uses the
<tt>_seed</tt> member to initialize the random sequences.

\subsection MPIimplementation_instruments The instruments

For an \em oligochromatic simulation, the stellar emission phase is immediately followed by the writing phase, where
the instruments and the dust system write out their information. There are different instruments, all inheriting from
the Instrument class. They all have their own private data members, which are arrays that represent the acquired
fluxes. The number of such arrays differs from one instrument subclass to the other. They also differ in name and size.

For the \ref Parallelizationmodes_taskbased mode, the instruments contain data for all wavelengths on every process,
and must therefore sum their values together and store the result at one single process, which then writes out the
information. When \ref Parallelizationmodes_datapar is used, some exta space needs to be allocated at a process, so
that it can gather all the pieces of the instrument and write the result for all wavelengths.

For summing SEDs contained in a instrument, we have implemented a function, called \c sumResults, in the Instrument
base class that is responsible for summing the values in a certain list of flux arrays across the different processes.
Some subclasses have only one such list, other instruments have more. Therefore, each subclass calls the \c sumResults
function a different number of times. The choice of implementing the summing procedure in the shared Instrument base
class over implementing that procedure in each derived class separately provides clarity, improves modularity (adding
new instrument classes) and prevents needless duplication of code.

The implementation of the \c sumResults function in Instrument is as follows:

\code
void Instrument::sumResults(QList< Array*> arrays)
{
    PeerToPeerCommunicator * comm = find<PeerToPeerCommunicator>();
    foreach (Array* arr, arrays) comm->sum(*arr);
}
\endcode

A pointer to the PeerToPeerCommunicator instance of the simulation hierarchy is obtained by using the \c find
operation. For each (flux) array in the list passed to \c sumResults, the \c sum function of the PeerToPeerCommunicator
class is called. The sum function makes sure that the values in the array pointed to by \c arr are summed element-wise
across the processes, and that the results are stored in the <em>original array in the memory of the process with rank
zero</em>. Another function of PeerToPeerCommunicator, \c sum_all can be used to apply a summing procedure where each
process obtains the results afterwards. The sum and sum_all function of the ProcessManager class are just simple
wrappers around the \c MPI_Reduce and \c MPI_Allreduce functions respectively.

Most of the instruments also contain one or multiple data cubes, blocks of data that store an image of the flux for
every wavelength. Because the necessary behaviour is different for the two parallelization modes, a class called \c
ParallelDataCube has been created. Depending on the mode used, an object of this class will either store a data cube
containing all the wavelengths of a simulation, or one that only stores the wavelengths relevant for each process. This
class also contains a function called \c constructCompleteCube, which either sums the data cube over all processes in
task-based mode, or gathers and assembles the partial data cubes stored on the processes in data parallel mode. The
result of this operation is stored in an Array at the root process, which can then perform some operations and write
out the image.

The implementation of \c constructCompleteCube uses the the function \c gatherw of the PeerToPeerCommunicator. The
backbone of this function is \c MPI_Alltoallw, a very general collective communication function of MPI. With \c
gatherw, blocks of data can be gathered from different processes into arbitrary locations within the receive buffer. By
giving the correct arguments, all the wavelength slices of a data cube can be gathered at a single process and put into
the right locations in memory with a single MPI call.

The Instrument subclasses (SimpleInstrument, SEDInstrument, FullInstrument, FrameInstrument) contain a function \c
write, which is called from the InstrumentSystem class for each instrument in the simulation. In this function, each
instrument places pointers to the flux arrays it needs to write out (type Array) into a list of type \c QList. This is
typically a list called \c farrays for their (potential) data cube arrays, and a list called \c Farrays for their
(potential) SED data arrays. These two lists (if present) are then respectively passed to the functions \c
calibrateAndWriteDataCubes and \c calibrateAndWriteSEDs. These functions are implemented in the SingleFrameInstrument
and DistantInstrument classes, from which the concrete instrument classes inherit. After the calls to the \c sumResults
and \c ParallelDataCube::constructCompleteCube functions have returned, these functions convert the flux data into
appropriate units and subsequently write this data to file.

For the panchromatic simulations, the algorithm for the instruments is identical, but the writing phase is not entered
before the dust selfabsorption and dust emission phases.

\subsection MPIimplementation_emissioncalculation The dust emission calculation

\subsubsection emissioncalculation_absorptiondata The absorption data

For a \em panchromatic simulation, extra phases are involved before the writing phase. These phases involve a repeated
simulation of (stellar or thermal) photon packages during which the absorbed luminosities in the dust cells should be
stored. Before the implementation of the data parallelization, the PanDustSystem class stored the absorption
information in the form of two container variables: \c _Labsstelvv and \c _Labsdustvv. These containers used to be of
the type Table<2>, i.e. a two-dimensional instance of the template class Table. An instance of Table can be constructed
by specifying its dimensions \f$n_0\f$ and \f$n_1\f$ and its elements can be accessed by specifying two indices (i,j).
Internally, it consists of an Array of size \f$n_0 \times n_1\f$.

This absorption information is used when the \c calculatedustemission function of the PanDustSystem class is called. To
perform this calculation, every process will need a certain set of data, depending on the parallelization mode: For
task-based mode, every process needs all absorption data, and this data can be obtained by summing the absorption table
over all the processes. For the data-parallel mode, every process needs absorption data for the subset of the dust
cells it will work on, and a transposition from a wavelength-based data parallelization scheme to a dustcell-based
scheme has to happen.

To encapsulate the differences between these two cases, the Tables containing the absorption data have been replaced by
instances of \c ParallelTable. An instance of this class can be initialized to behave either as a regular table or as a
parallelized data container, where only certain rows or columns are stored (see \c ParallelTable::initialize()). The
class also provides the \c switchScheme() function. When the object has been initialized as a regular table, this
function will sum the contributions over all processes. When the data-parallel mode is used, it will perform the
transposition we need by using \c alltoallw of the \c PeerToPeerCommunicator, a wrapper around \c MPI_Alltotallw. Using
the latter function, the whole transposition routine can be performed in a single MPI call. For detailed information
about what a \c ParallelTable can and cannot do, refer to the class documentation.

The code below shows how \c _Labsstelvv and \c _Labsdustvv are initialized during the setup of PanDustSystem. The
assigners used to initialize them in data-parallel mode are members of the WavelengthGrid and the PanDustSystem, to
determine the column-wise and row-wise distribution schemes respectively. By making these assigners members of
important simulation objects, the distribution schemes for the wavelengths and dust cells gain a more global character.
In any part of the code, the WavelengthGrid or PanDustSystem can be found using the \c find<T>() function, and
subsequently their \c assigner() getter can be used to obtain a pointer to the ProcessAssigner for the wavelenghts or
the dust cells respectively. By using these assigners, it can be made sure that operations that write to (or read from)
a ParallelTable, performed through a \c Parallel::call(), only need access to the elements that are actually available.

\code
PeerToPeerCommunicator* comm = find<PeerToPeerCommunicator>();
bool dataParallel = comm->dataParallel();

if (dataParallel) // assign this process to work with a subset of dust cells
    _assigner = new StaggeredAssigner(_Ncells, this);

WavelengthGrid* wg = find<WavelengthGrid>();

// resize the tables that hold the absorbed energies for each dust cell and wavelength
// - absorbed stellar emission is relevant for calculating dust emission
// - absorbed dust emission is relevant for calculating dust self-absorption
_haveLabsStel = false;
_haveLabsDust = false;
if (dustemission())
{
    if (dataParallel)
        _LabsStelvv.initialize("Absorbed Stellar Luminosity Table", ParallelTable::WriteState::COLUMN,
                               wg->assigner(), _assigner, comm);
    else
        _LabsStelvv.initialize("Absorbed Stellar Luminosity Table", ParallelTable::WriteState::COLUMN,
                               _Nlambda, _Ncells, comm);
    _haveLabsStel = true;

    if (selfAbsorption())
    {
        if (dataParallel)
            _LabsDustvv.initialize("Absorbed Dust Luminosity Table", ParallelTable::WriteState::COLUMN,
                                   wg->assigner(), _assigner, comm);
        else
            _LabsDustvv.initialize("Absorbed Dust Luminosity Table", ParallelTable::WriteState::COLUMN,
                                   _Nlambda, _Ncells, comm);
        _haveLabsDust = true;
    }
}
\endcode

In this part of the setup, it is first determined if the data-parallel mode is active, by obtaining a boolean from the
PeerToPeerCommunicator. If this is the case, the assigner for the dust cells is created, by giving the number of dust
cells as a constructor argument. Then, the absorption tables are initialized, providing a name, and an initial
parallelization scheme (column-based). The next two arguments are two assigners; the distribution scheme for the
columns is set equal to that of the wavelengths by passing the assigner member of the WavelengthGrid. The same happens
for our alternate, row-based scheme, by passing the assigner for the dust cells we just created. Without data
parallelization, these arguments are simply integers, causing the ParallelTable to behave like a regular Table. The
ParallelTable will use the last argument, a pointer to the communicator, to perform \c switchScheme() and several other
functions involving MPI communications.

There are two different circumstances in which we want to calculate the dust emission. The first is when we have just
launched stellar photon packages, and we want to calculate the emission based on the absorption of these photon
packages (in \c _Labsstelvv) to initiate the first dust selfabsorption cycle. The other situation happens after each
dust selfabsorption cycle, where the absorption luminosities of thermal photon packages get updated. In this case, we
want to recalculate the dust emission because of these updated thermal absorptions (stored in \c _Labsdustvv), whereas
the absorption luminosities of stellar photon packages (stored in \c _Labsstelvv) have not been altered. Thus, in the
first situation we want to communicate the values in \c _Labsstelvv between the processes, whereas in the second
situation we need to communicate the values of \c _Labsdustvv between the processes. This behaviour is built-in into \c
ParallelTable: \c switchScheme can only be performed once, and will only perform an MPI operation when the contents of
the ParallelTable have been altered. If the ParallelTable has not been written to since its initialization, the
dimensions of the storage will be changed, but no communication will be nessecary, as the contents will consist
entirely of zeros.

The dust selfabsorption phase consists of 3 \em stages. During each such stage, a simulation of thermal photon packages
is performed repeatedly (in so-called \em cycles), until a certain convergence is reached on the total absorbed
luminosity of thermal photon packages. The first stage, second stage and third stage respectively launch 1/10, 1/3 and
1 times the number of photon packages used for the stellar emission phase. A stage is finished when the total absorbed
thermal luminosity convergence or the maximum number of cycles (which is 100) is reached. Only in the <em>first cycle
of the first stage</em>, the absorbed \em stellar luminosities have to be communicated between the different processes.
Once the summed stellar luminosities are stored in each processor’s memory, these values are not changed anymore during
the simulation. The following statement is implemented in the body of the \c
PanMonteCarloSimulation::rundustselfabsorption loop:

\code
_pds->calculatedustemission();
\endcode

The variable \c _pds is a pointer to the dust system used for panchromatic simulations. The calculatedustemission
function does the following:

- First, it checks whether dust emission is turned on or off. If it is turned off (one can choose to do so in the \em
ski file), then the function immediately returns (it does nothing).

- If dust emission is enabled, the \c sumResults function of the same class is called, which performs the
communication and potential switch of parallelization scheme by calling \c switchScheme on both ParallelTables

- After \c sumResults, the actual calculation is executed. This is done by calling the calculate function of the
\c _dustlib object, an instance of one of the DustLib subclasses.

\note A panchromatic simulation does not necessarily include a dust selfabsorption phase. While enabled by default, the
execution of this phase can be disabled in the \em ski file for the simulation. In this case, the stellar emission
phase is immediately followed by the dust emission phase. The dust emission calculation and the accompanying
communications are performed only once, and no absorption is recorded while shooting the photons for the dust emission.

\subsubsection emissioncalculation_library The dust library

After communicating the luminosities, each process invokes a procedure to actually calculate the dust emission spectra.
This procedure is the \c calculate function of the dust library object, an instance of the DustLib class (or rather a
DustLib subclass). The DustLib object is the object that delegates the calculation of the dust emission spectra in all
dust cells, using the absorption information contained in the PanDustSystem, and for that purpose taking advantage of a
library mechanism. Without going into detail here (refer to the DustLib class reference), this mechanism allows
<tt>SKIRT</tt> to determine an approximate dust emission spectrum appropriate for different cells at once, instead of
doing the calculation separately (and more exact) for each individual dust cell. A set of dust cells for which the
emission spectrum is calculated at once, is represented by a so-called <em>library entry</em>.

Such a library entry is thus the <em>unit of parallelization</em> for the dust emission calculation. The
parallelization of these different entries with multithreading is handled by the Parallel class in an unpredictable
manner, similar as the simulation of chunks of photon packages. For the MPI parallelization of this procedure, we have
to define some way in which different processes are assigned to different library entries. For this purpose, we can use
an instance of any of the subclasses of \c ProcessAssigner. The DustLib class does not need to know which kind of
assignment mechanism is used, it leaves this task up to the dedicated ProcessAssigner object, and only uses this object
through the interface of the ProcessAssigner base class.

The dust emission calculation, performed by the \c calculate function of DustLib, begins with determining the number of
library entries and calculating the mapping from dust cells to library entries. Here, an important distinction between
the two parallelization modes arises. To construct this mapping, the dustlib needs the absorbed luminosity for all dust
cells. The exception to this rule is the trivial implementation of a DustLib called the \c AllCellsDustLib, which
simply creates a one-to-one mapping from dust cells to library entries, and hence does not need any absorption data at
all to generate its mapping. When data parallelization is used, the absorption data is only available for a subset of
the dustcells on each process. As a consequence, only the \c AllCellsDustLib can be used with data parallelization.

Closely related to this distinction is the part where we decide which ProcessAssigner object we need to use. In the
task-based parallelization mode, we simply create a new StaggeredAssigner that distributes whatever number of library
entries between the processes (this number is given to the constructor). Each process can then start calculating the
emission template for the library entries it was assigned to. Under data parallelization, we first repeat that only an
AllCellsDustLib can be used, and hence the library entries map directly to individual cells. This is important because
only the spectra of the cells for which the absorption data is available should be calculated. To ensure that this is
the case, the ProcessAssigner we choose has to be the same one that was used to initialize the ParallelTable containing
the necessary data. This assigner is stored as a member of the PanDustSystem, and is constructed during the setup phase
given that the \c '-d' command line argument was present.

Listed below is the implementation (with some omissions for readability) of the \c calculate function in the DustLib
base class. A boolean indicating whether or not data parallelization is active, is obtained from the \c
PeerToPeerCommunicator. If needed, the assigner that determines the parallelization scheme of the dust cells can be
obtained through the PanDustSystem.

\code
void DustLib::calculate()
{
    // get mapping linking cells to library entries.
    int Nlib = entries();
    _nv = mapping();

    // calculate the emissivity for each library entry
    EmissionCalculator calc(_Lvv, _nv, Nlib, this);
    Parallel* parallel = find<ParallelFactory>()->parallel();

    PanDustSystem* ds = find<PanDustSystem>();
    PeerToPeerCommunicator* comm = find<PeerToPeerCommunicator>();
    bool dataParallel = comm->dataParallel();
    if (dataParallel)
    {
        // Each process only has data for a subset of dust cells.
        // The available dust cells are indicated by the cell assigner.
        // The call below calculates the emission for those cells.
        // ONLY WORKS FOR ALLCELLSDUSTLIB AND THIS IS INTENDED
        parallel->call(&calc, ds->assigner());
    }
    else
    {
        // The processes have access to all the cells,
        // and can hence use library subclasses other than AllCellsDustLib.
        // Divide the work over the processes per library entry, using an auxiliary assigner.
        if (!_libAssigner) _libAssigner = new StaggeredAssigner(Nlib, this);
        parallel->call(&calc, _libAssigner);
    }
    _Lvv.switchScheme();
}
\endcode

The actual calculation is initiated on each process by creating an object of the EmissionCalculator class and calling
its \c body function for as many times as there are library entries assigned to the process. This \c body function
takes the index \f$n\f$ of a library entry as an argument an subsequently calculates the emission spectrum for this
library entry. If there is only one dust component, it stores this spectrum in a list <tt>_Lvv</tt> indexed on \f$n\f$
(the library entries) at the corresponding index. If there are multiple dust components, the <tt>_Lvv</tt> list is
indexed on \f$m\f$, the dust cells, and the average ISRF for library entry \f$n\f$ is used to calculate the different
emission spectra for the dust cells that map to it, and these spectra are stored in the corresponding places in the
<tt>_Lvv</tt> list. The loop over the different library entries \f$n\f$ for which the \c body function of
EmissionCalculator is called, is handled by the Parallel object. The <tt>call</tt> function of Parallel is given a
pointer to a ProcessAssigner as one of its argumenst. The Parallel object will then use the pointer to the
ProcessAssigner to determine which indices it has to pass to the body function of EmissionCalculator, i.e. which
library entries are assigned to the process. For this subset of library entries, the calculation is also parallelized
between different threads.

The last thing that the <tt>calculate</tt> function does is of course sharing the calculated spectra with the other
processes. The data structure containing the result of this calculation \c _Lvv can also have a significant size, of
either \f$N_\text{library entries} \times N_\lambda\f$ or \f$N_\text{dust cells} \times N_\lambda\f$. Thats why this
list of spectra has been parallelized almost completely analogously to the absorption tables of PanDustSystem, by using
a ParallelTable for \c _Lvv. The only difference is that we start from a scheme where each process stores several rows,
which can be specified as an option in the initialization function of \c ParallelTable. The communication of the
spectra then simply comes down to calling \c switchScheme on \c _Lvv. This will either sum the results over all
processes, filling in the missing spectra for each process; or it will perform a transposition from a dustcell-based to
a wavelength-based parallelization of data storage.

At this point, each process contains the emission spectrum for each dust cell or library entry, for the wavelengths it
needs. The PanMonteCarloSimulation can thus proceed with the next cycle of shooting thermal photon packages from the
dust system, based on these spectra.

\subsection MPIimplementation_logging Logging

<tt>SKIRT</tt> uses an advanced logging mechanism that clearly marks the beginning and end of the different simulation
phases, provides timestamps to the log messages, and allows for two (or possibly more) Log objects to be linked, so
that the parent (in case of <tt>SKIRT</tt>, the <em>console logger</em>) passes the messages to the child (the <em>file
logger</em>).

The console logger, an instance of the Console class – inheriting from Log – is created by its default constructor
during the construction of the SkirtCommandLineHandler object. After its construction, the console logger prints a
welcome message.

When the \c perform function of the SkirtCommandLineHandler class is called, <tt>SKIRT</tt> determines whether to start
a (series of) simulation(s) or execute its <em>interactive mode</em> where the user can create a \em ski file. Because
the interactive mode is very basic and depends on sequential user input, it is not parallelized in any way. Therefore,
we have taken care that if <tt>SKIRT</tt> is somehow started in interactive mode (for example by not providing any
command line arguments) <em>with MPI</em>, the program is shut down with an error message. This is done by calling the
\c isMultiProc function of the ProcessManager class at the beginning of the doInteractive function, and throwing a
<tt>FATALERROR</tt> when the former returns true.

In <em>simulation mode</em>, the console prints out that it is constructing a simulation for the specified
<em>ski</em>-file(s). After that, an object of the Simulation class is constructed. Using its default constructer, a
new object of the Console class is created as a data member. This object will be solely responsible for all further
logging during the simulation. Another logger, of the class FileLog, is linked to the Console logger and writes out the
exact same messages to a file during the simulation.

The actual simulation is executed by calling the \c setupAndRun function of the Simulation class. This function calls
two other functions, \c setup and \c run. All of this is not important without MPI, but if we do use multiple
processes, we want the logging mechanism to adapt to that situation. Imagine that we don’t change anything in the
logging algorithm. Then we would have a whole bunch of processes, all thinking they are executing an independent
simulation, logging messages to the terminal and to a file. This would result in a mess of redundant information. On
the other hand, it could be useful for some types of messages to always reach the user, irrespective of which process
wants to log them. Error messages, for example, are not necessarily expected to occur in each process at the same time.
It is therefore certainly desired to always output these messages; otherwise the program could crash with no apparent
reason.

It is obvious that different kinds of messages require a different treatment. There are, in <tt>SKIRT</tt>, four kinds
of messages: info, warning, success and error. Info messages are used for showing the progress of the simulation(s) and
providing general info about the simulation name, <tt>SKIRT</tt> version etc. Warnings are used when unexpected
behaviour occurs somewhere during the simulation, but it can be solved or handled without affecting the results or
performance of the program. Still, the user may want to be informed by the unexpected event. Errors are similar,
however, the program should usually be terminated after an error is encountered. A success message is not very
different from an info message, but it is used to inform the user of the successful ending of a particular simulation
phase. Since the different processes execute the same algorithm, the info and success messages they intend to show are
identical. Subsequently, it is sufficient to let only one process, the root process, output these messages to the
console and to file. Thus, the following line is implemented at the beginning of the \c info and \c success function of
the Log class:

\code
if (!ProcessManager::isRoot()) return;
\endcode

Therefore, the Log::info and Log::success function do nothing for a process that is not the root, and otherwise they
perform as usual. In the warning and error function, the above line is not added. In these functions, an additional
variable \c _procName is used. This is object of type \c QString, and is a data member of the Log class. During the
initialization of a Log object, its value is defaulted to the empty string. Its value can be changed afterwards by
calling the Log::setProcessName function. This function takes the rank i of the process (an int) as a parameter, and
sets the process name to "[Process i]". The Log::setProcessName function is called in the setup of the Log baseclass,
after a pointer to the ProcessCommunicator object is obtained and it is verified that this object has been set up
itsself:

\code
void Log::setupSelfBefore()
{
    ProcessCommunicator* comm;

    try
    {
        // get a pointer to the ProcessCommunicator without performing setup
        // to avoid catching (and hiding) fatal errors during such setup
        comm = find<ProcessCommunicator>(false);
    }
    catch (FatalError)
    {
        return;
    }

    // Do the find operation again, now to perform the setup of the
    // PeerToPeerCommunicator so that the correct rank is initialized
    comm = find<ProcessCommunicator>();

    if (comm->isMultiProc()) setProcessName(comm->rank());
}
\endcode

The Log::setProcessName function automatically calls itself also for the linked log. When Log::warning or Log::error is
called afterwards, the process name is shown after the timestamp and before the actual message. We have also added the
process name to the info and success log messages, when <tt>SKIRT</tt> is compiled in debug mode. In this mode, these
messages are then always shown, irrespective of the process. This allows for the user to find out which process causes
the program to hang, for example.

*/
